(window.webpackJsonp=window.webpackJsonp||[]).push([[16],{109:function(e,t,n){"use strict";n.d(t,"a",(function(){return f})),n.d(t,"b",(function(){return p}));var i=n(0),a=n.n(i);function l(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){l(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function c(e,t){if(null==e)return{};var n,i,a=function(e,t){if(null==e)return{};var n,i,a={},l=Object.keys(e);for(i=0;i<l.length;i++)n=l[i],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(i=0;i<l.length;i++)n=l[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=a.a.createContext({}),d=function(e){var t=a.a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},f=function(e){var t=d(e.components);return a.a.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.a.createElement(a.a.Fragment,{},t)}},m=a.a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,l=e.originalType,r=e.parentName,s=c(e,["components","mdxType","originalType","parentName"]),f=d(n),m=i,p=f["".concat(r,".").concat(m)]||f[m]||u[m]||l;return n?a.a.createElement(p,o(o({ref:t},s),{},{components:n})):a.a.createElement(p,o({ref:t},s))}));function p(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var l=n.length,r=new Array(l);r[0]=m;var o={};for(var c in t)hasOwnProperty.call(t,c)&&(o[c]=t[c]);o.originalType=e,o.mdxType="string"==typeof e?e:i,r[1]=o;for(var s=2;s<l;s++)r[s]=n[s];return a.a.createElement.apply(null,r)}return a.a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},112:function(e,t,n){"use strict";n.d(t,"b",(function(){return l})),n.d(t,"a",(function(){return r}));var i=n(22),a=n(114);function l(){var e=Object(i.default)().siteConfig,t=(e=void 0===e?{}:e).baseUrl,n=void 0===t?"/":t,l=e.url;return{withBaseUrl:function(e,t){return function(e,t,n,i){var l=void 0===i?{}:i,r=l.forcePrependBaseUrl,o=void 0!==r&&r,c=l.absolute,s=void 0!==c&&c;if(!n)return n;if(n.startsWith("#"))return n;if(Object(a.b)(n))return n;if(o)return t+n;var d=n.startsWith(t)?n:t+n.replace(/^\//,"");return s?e+d:d}(l,n,e,t)}}}function r(e,t){return void 0===t&&(t={}),(0,l().withBaseUrl)(e,t)}},114:function(e,t,n){"use strict";function i(e){return!0===/^(\w*:|\/\/)/.test(e)}function a(e){return void 0!==e&&!i(e)}n.d(t,"b",(function(){return i})),n.d(t,"a",(function(){return a}))},86:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return o})),n.d(t,"metadata",(function(){return c})),n.d(t,"toc",(function(){return s})),n.d(t,"default",(function(){return f}));var i=n(3),a=n(7),l=(n(0),n(109)),r=n(112),o={id:"doc8",title:"Calibration using experimental results"},c={unversionedId:"doc8",id:"doc8",isDocsHomePage:!1,title:"Calibration using experimental results",description:"Calibration concept",source:"@site/docs/doc8.md",slug:"/doc8",permalink:"/Robyn/docs/doc8",editUrl:"https://github.com/facebookexperimental/Robyn/docs/doc8.md",version:"current",sidebar:"someSidebar",previous:{title:"Automated hyperparameter selection and optimization",permalink:"/Robyn/docs/doc7"},next:{title:"Outputs and diagnostics",permalink:"/Robyn/docs/doc9"}},s=[{value:"Calibration concept",id:"calibration-concept",children:[]},{value:"Calibration in the code",id:"calibration-in-the-code",children:[]}],d={toc:s};function f(e){var t=e.components,n=Object(a.a)(e,["components"]);return Object(l.b)("wrapper",Object(i.a)({},d,n,{components:t,mdxType:"MDXLayout"}),Object(l.b)("h3",{id:"calibration-concept"},"Calibration concept"),Object(l.b)("p",null,"By applying results from randomized controlled-experiments, you may improve the accuracy of your marketing mix models dramatically. It is recommended to run these on a recurrent basis to keep the model calibrated permanently. In general, we want to compare the experiment result with the MMM estimation of a marketing channel. Conceptually, this method is like a Bayesian method, in which we use experiment results as a prior to shrink the coefficients of media variables. A good example of these types of experiments is Facebook\u2019s conversion lift tool which can help guide the model towards a specific range of incremental values."),Object(l.b)("img",{alt:"Calibration chart",src:Object(r.a)("/img/calibration1.png")}),Object(l.b)("p",null,"Figure illustrates the calibration process above for one MMM candidate model.\nTable below illustrates the model selection output including FB lift calibration element. Modelers can select the top models with relatively small MAPE metrics as the candidates for the final model. In this example, we suggest picking model two, as it has the minimum ",Object(l.b)("em",null,"MAPE(cal,fb)")," and its ",Object(l.b)("em",null,"MAPE(holdout)")," is only 0.4% more than the minimum one."),Object(l.b)("h4",{id:"example-table"},"Example Table"),Object(l.b)("p",null,"Sample output of model selection of a MMM with only two media channels, TV and Social"),Object(l.b)("img",{alt:"Calibration table",src:Object(r.a)("/img/calibration2.png")}),Object(l.b)("p",null,"Note that ",Object(l.b)("em",null,"MAPE(cal,fb)")," will likely vary more widely than ",Object(l.b)("em",null,"MAPE(holdout)")," . Given this, calibration can improve performance without substantially sacrificing backtesting performance.\nThis calibration method can be applied to other media channels which run experiments, the more channels that are calibrated, the more accurate the MMM model. ",Object(l.b)("em",null,"You may find the calibration function in the \u2018func.R\u2019 script.")),Object(l.b)("h3",{id:"calibration-in-the-code"},"Calibration in the code"),Object(l.b)("p",null,"So, how do we apply this in our code?"),Object(l.b)("ol",null,Object(l.b)("li",{parentName:"ol"},"First, we check if media channels to be calibrated actually have a media variable created."),Object(l.b)("li",{parentName:"ol"},"After that, we collect all different media to be calibrated. Consequently, we loop over each lift channel (Where for each of them we iterate over all different studies if more than one, determining the date range of each study)"),Object(l.b)("li",{parentName:"ol"},"In addition, we convert data from weeks to days (Please note the *7 in the formula for mmmDays, this is assuming you will use weekly data as a basis for your model)."),Object(l.b)("li",{parentName:"ol"},"Finally, and once both lift study and MMM dates are both in days, we scale the total decomposed model predicted sales into the exact number of days the lift study had to be comparable with previously uploaded liftAbs number under the set_lift variable (remember liftAbs values in set_lift variable have to be absolute and measuring the same metric as the model does ie. total incremental sales vs. model predicted sales)")),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre"},'#### Define lift calibration function\nf.calibrateLift <- function(decompCollect, set_lift) {\n\n  check_set_lift <- any(sapply(set_lift$channel, function(x) any(str_detect(x, set_mediaVarName)))==F) #check if any lift channel doesn\'t have media var\n  if (check_set_lift) {stop("set_lift channels must have media variable")}\n  ## prep lift input  \n  getLiftMedia <- unique(set_lift$channel)\n  getDecompVec <- decompCollect$xDecompVec\n\n  ## loop all lift input\n  liftCollect <- list()\n  for (m in 1:length(getLiftMedia)) { # loop per lift channel\n\n    liftWhich <- str_which(set_lift$channel, getLiftMedia[m])\n\n    liftCollect2 <- list()\n    for (lw in 1:length(liftWhich)) { # loop per lift test per channel\n\n      ## get lift period subset\n      liftStart <- set_lift[liftWhich[lw], liftStartDate]\n      liftEnd <- set_lift[liftWhich[lw], liftEndDate]\n      liftPeriodVec <- getDecompVec[DS >= liftStart & DS <= liftEnd, c("DS", getLiftMedia[m]), with = F]\n\n      ## scale decomp\n      mmmDays <- nrow(liftPeriodVec) * 7\n      liftDays <- as.integer(liftEnd- liftStart + 1)\n      y_hatLift <- sum(unlist(getDecompVec[, -1])) # total pred sales\n      x_decompLift <- sum(liftPeriodVec[,2])\n      x_decompLiftScaled <- x_decompLift / mmmDays * liftDays\n\n      ## output\n      liftCollect2[[lw]] <- data.table(liftMedia = getLiftMedia[m] ,\n                                       liftStart = liftStart,\n                                       liftEnd = liftEnd,\n                                       liftAbs = set_lift[liftWhich[lw], liftAbs],\n                                       decompAbsScaled = x_decompLiftScaled)\n    }\n    liftCollect[[m]] <- rbindlist(liftCollect2)\n  }\n')),Object(l.b)("p",null,"The last step is to calculate the MAPE. This will be the key metric to define the model that is closest to actual incremental sales during periods for the lift study. It will therefore allow us to make a decision as per the example on the ",Object(l.b)("a",{parentName:"p",href:"#example-table"},Object(l.b)("strong",{parentName:"a"},"table")),"."),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre"},"  ## get mape_lift\n  liftCollect <- rbindlist(liftCollect)[, mape_lift := abs((decompAbsScaled - liftAbs) / liftAbs) * 100]\n  return(liftCollect)\n}\n")))}f.isMDXComponent=!0}}]);