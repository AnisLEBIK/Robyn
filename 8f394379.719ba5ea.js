(window.webpackJsonp=window.webpackJsonp||[]).push([[11],{81:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return o})),n.d(t,"metadata",(function(){return c})),n.d(t,"toc",(function(){return s})),n.d(t,"default",(function(){return u}));var i=n(3),a=n(7),l=(n(0),n(88)),r=n(90),o={id:"calibration",title:"Calibration using experimental results"},c={unversionedId:"calibration",id:"calibration",isDocsHomePage:!1,title:"Calibration using experimental results",description:"Calibration concept",source:"@site/docs/calibration.md",slug:"/calibration",permalink:"/Robyn/docs/calibration",editUrl:"https://github.com/facebookexperimental/Robyn/docs/calibration.md",version:"current",sidebar:"someSidebar",previous:{title:"Automated hyperparameter selection and optimization",permalink:"/Robyn/docs/automated-hyperparameter-selection-optimization"},next:{title:"Outputs and diagnostics",permalink:"/Robyn/docs/outputs-diagnostics"}},s=[{value:"Calibration concept",id:"calibration-concept",children:[]},{value:"Calibration in the code",id:"calibration-in-the-code",children:[]}],f={toc:s};function u(e){var t=e.components,n=Object(a.a)(e,["components"]);return Object(l.b)("wrapper",Object(i.a)({},f,n,{components:t,mdxType:"MDXLayout"}),Object(l.b)("h3",{id:"calibration-concept"},"Calibration concept"),Object(l.b)("p",null,"By applying results from randomized controlled-experiments, you may improve the\naccuracy of your marketing mix models dramatically. It is recommended to run\nthese on a recurrent basis to keep the model calibrated permanently. In general,\nwe want to compare the experiment result with the MMM estimation of a marketing\nchannel. Conceptually, this method is like a Bayesian method, in which we use\nexperiment results as a prior to shrink the coefficients of media variables. A\ngood example of these types of experiments is Facebook\u2019s conversion lift tool\nwhich can help guide the model towards a specific range of incremental values."),Object(l.b)("img",{alt:"Calibration chart",src:Object(r.a)("/img/calibration1.png")}),Object(l.b)("p",null,"Figure illustrates the calibration process above for one MMM candidate model.\nTable below illustrates the model selection output including FB lift calibration\nelement. Modelers can select the top models with relatively small MAPE metrics\nas the candidates for the final model. In this example, we suggest picking model\ntwo, as it has the minimum ",Object(l.b)("em",null,"MAPE(cal,fb)")," and its ",Object(l.b)("em",null,"MAPE(holdout)"),"\nis only 0.4% more than the minimum one."),Object(l.b)("h4",{id:"example-table"},"Example Table"),Object(l.b)("p",null,"Sample output of model selection of a MMM with only two media channels, TV and\nSocial ",Object(l.b)("img",{alt:"Calibration table",src:Object(r.a)("/img/calibration2.png")})),Object(l.b)("p",null,"Note that ",Object(l.b)("em",null,"MAPE(cal,fb)")," will likely vary more widely than"),Object(l.b)("em",null,"MAPE(holdout)")," . Given this, calibration can improve performance without substantially sacrificing backtesting performance. This calibration method can be applied to other media channels which run experiments, the more channels that are calibrated, the more accurate the MMM model. ",Object(l.b)("em",null,"You may find the calibration function in the \u2018func.R\u2019 script."),Object(l.b)("h3",{id:"calibration-in-the-code"},"Calibration in the code"),Object(l.b)("p",null,"So, how do we apply this in our code?"),Object(l.b)("ol",null,Object(l.b)("li",{parentName:"ol"},"First, we check if media channels to be calibrated actually have a media\nvariable created."),Object(l.b)("li",{parentName:"ol"},"After that, we collect all different media to be calibrated. Consequently, we\nloop over each lift channel (Where for each of them we iterate over all\ndifferent studies if more than one, determining the date range of each study)"),Object(l.b)("li",{parentName:"ol"},"In addition, we convert data from weeks to days (Please note the ","*","7 in the\nformula for mmmDays, this is assuming you will use weekly data as a basis for\nyour model)."),Object(l.b)("li",{parentName:"ol"},"Finally, and once both lift study and MMM dates are both in days, we scale\nthe total decomposed model predicted sales into the exact number of days the\nlift study had to be comparable with previously uploaded liftAbs number under\nthe set_lift variable (remember liftAbs values in set_lift variable have to\nbe absolute and measuring the same metric as the model does ie. total\nincremental sales vs. model predicted sales)")),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre"},'#### Define lift calibration function\nf.calibrateLift <- function(decompCollect, set_lift) {\n\n  check_set_lift <- any(sapply(set_lift$channel, function(x) any(str_detect(x, set_mediaVarName)))==F) #check if any lift channel doesn\'t have media var\n  if (check_set_lift) {stop("set_lift channels must have media variable")}\n  ## prep lift input\n  getLiftMedia <- unique(set_lift$channel)\n  getDecompVec <- decompCollect$xDecompVec\n\n  ## loop all lift input\n  liftCollect <- list()\n  for (m in 1:length(getLiftMedia)) { # loop per lift channel\n\n    liftWhich <- str_which(set_lift$channel, getLiftMedia[m])\n\n    liftCollect2 <- list()\n    for (lw in 1:length(liftWhich)) { # loop per lift test per channel\n\n      ## get lift period subset\n      liftStart <- set_lift[liftWhich[lw], liftStartDate]\n      liftEnd <- set_lift[liftWhich[lw], liftEndDate]\n      liftPeriodVec <- getDecompVec[DS >= liftStart & DS <= liftEnd, c("DS", getLiftMedia[m]), with = F]\n\n      ## scale decomp\n      mmmDays <- nrow(liftPeriodVec) * 7\n      liftDays <- as.integer(liftEnd- liftStart + 1)\n      y_hatLift <- sum(unlist(getDecompVec[, -1])) # total pred sales\n      x_decompLift <- sum(liftPeriodVec[,2])\n      x_decompLiftScaled <- x_decompLift / mmmDays * liftDays\n\n      ## output\n      liftCollect2[[lw]] <- data.table(liftMedia = getLiftMedia[m] ,\n                                       liftStart = liftStart,\n                                       liftEnd = liftEnd,\n                                       liftAbs = set_lift[liftWhich[lw], liftAbs],\n                                       decompAbsScaled = x_decompLiftScaled)\n    }\n    liftCollect[[m]] <- rbindlist(liftCollect2)\n  }\n')),Object(l.b)("p",null,"The last step is to calculate the MAPE. This will be the key metric to define\nthe model that is closest to actual incremental sales during periods for the\nlift study. It will therefore allow us to make a decision as per the example on\nthe ",Object(l.b)("a",{parentName:"p",href:"#example-table"},Object(l.b)("strong",{parentName:"a"},"table")),"."),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre"},"  ## get mape_lift\n  liftCollect <- rbindlist(liftCollect)[, mape_lift := abs((decompAbsScaled - liftAbs) / liftAbs) * 100]\n  return(liftCollect)\n}\n")))}u.isMDXComponent=!0},88:function(e,t,n){"use strict";n.d(t,"a",(function(){return u})),n.d(t,"b",(function(){return b}));var i=n(0),a=n.n(i);function l(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){l(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function c(e,t){if(null==e)return{};var n,i,a=function(e,t){if(null==e)return{};var n,i,a={},l=Object.keys(e);for(i=0;i<l.length;i++)n=l[i],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(i=0;i<l.length;i++)n=l[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=a.a.createContext({}),f=function(e){var t=a.a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},u=function(e){var t=f(e.components);return a.a.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.a.createElement(a.a.Fragment,{},t)}},m=a.a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,l=e.originalType,r=e.parentName,s=c(e,["components","mdxType","originalType","parentName"]),u=f(n),m=i,b=u["".concat(r,".").concat(m)]||u[m]||d[m]||l;return n?a.a.createElement(b,o(o({ref:t},s),{},{components:n})):a.a.createElement(b,o({ref:t},s))}));function b(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var l=n.length,r=new Array(l);r[0]=m;var o={};for(var c in t)hasOwnProperty.call(t,c)&&(o[c]=t[c]);o.originalType=e,o.mdxType="string"==typeof e?e:i,r[1]=o;for(var s=2;s<l;s++)r[s]=n[s];return a.a.createElement.apply(null,r)}return a.a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},90:function(e,t,n){"use strict";n.d(t,"b",(function(){return l})),n.d(t,"a",(function(){return r}));var i=n(22),a=n(91);function l(){const{siteConfig:{baseUrl:e="/",url:t}={}}=Object(i.default)();return{withBaseUrl:(n,i)=>function(e,t,n,{forcePrependBaseUrl:i=!1,absolute:l=!1}={}){if(!n)return n;if(n.startsWith("#"))return n;if(Object(a.b)(n))return n;if(i)return t+n;const r=n.startsWith(t)?n:t+n.replace(/^\//,"");return l?e+r:r}(t,e,n,i)}}function r(e,t={}){const{withBaseUrl:n}=l();return n(e,t)}},91:function(e,t,n){"use strict";function i(e){return!0===/^(\w*:|\/\/)/.test(e)}function a(e){return void 0!==e&&!i(e)}n.d(t,"b",(function(){return i})),n.d(t,"a",(function(){return a}))}}]);